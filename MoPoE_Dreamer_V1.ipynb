{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN8Ohc0Mh8C"
      },
      "source": [
        "# 1.準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXOA_CGxNmTe"
      },
      "source": [
        "## importなど"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZaErbWsMOOB"
      },
      "outputs": [],
      "source": [
        "!pip install pybullet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72zXgw6dMZ3_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pybullet_envs  # PyBulletの環境をgymに登録する\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from einops import rearrange\n",
        "\n",
        "# 可視化のためにTensorBoardを用いるので，Colab上でTensorBoardを表示するための宣言を行う\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVXol8PNO74X"
      },
      "source": [
        "今回の学習にはGPUが必要です．\n",
        "\n",
        "以下のコードを実行して， 結果が'cuda'でなければ「ランタイム」 →　 「ランタイムのタイプを変更」でGPUモードに変更しましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM8S09VFO2Y_"
      },
      "outputs": [],
      "source": [
        "# torch.deviceを定義．この変数は後々モデルやデータをGPUに転送する時にも使います\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmOEoN-6PLkp"
      },
      "source": [
        "きちんとGPUが使える状態になっているかチェックしておきます． ColabのGPU割り当てにはランダム性があるので人によって結果が違う場合がありますが， どれでも実行には問題ありません（学習にかかる時間に幾らかの差は出ます）．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2XwZ-e9O-KM"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eayBK1HPV5H"
      },
      "source": [
        "# 2.環境の設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvrA4khsPelK"
      },
      "source": [
        "環境を扱いやすくするために，いくつかラッパーを挟みます．\n",
        "\n",
        "今回はPyBulletを画像入力の環境として用います．環境を作成して，画像がどのようになっているかを見てみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byt7c-EYPQP3"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"HalfCheetahBulletEnv-v0\")  # 環境を読み込む．\n",
        "env.reset()\n",
        "image = env.render(mode=\"rgb_array\")  # env.renderで画像を取得\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJSLGoytbN0P"
      },
      "outputs": [],
      "source": [
        "class GymWrapper_PyBullet(object):\n",
        "    \"\"\"\n",
        "    PyBullet環境のためのラッパー\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n",
        "    reward_range = (-np.inf, np.inf)\n",
        "\n",
        "    # __init__でカメラ位置に関するパラメータ（ cam_dist:カメラ距離，cam_yaw：カメラの水平面での回転，cam_pitch:カメラの縦方向での回転）を受け取り，カメラの位置を調整できるようにします.\n",
        "    # 　同時に画像の大きさも変更できるようにします\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        cam_dist: int = 3,\n",
        "        cam_yaw: int = 0,\n",
        "        cam_pitch: int = -30,\n",
        "        render_width: int = 320,\n",
        "        render_height: int = 240,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        env : gym.Env\n",
        "            gymで提供されている環境のインスタンス．\n",
        "        cam_dist : int\n",
        "            カメラの距離．\n",
        "        cam_yaw : int\n",
        "            カメラの水平面での回転．\n",
        "        cam_pitch : int\n",
        "            カメラの縦方向での回転．\n",
        "        render_width : int\n",
        "            観測画像の幅．\n",
        "        render_height : int\n",
        "            観測画像の高さ．\n",
        "        \"\"\"\n",
        "        self._env = env\n",
        "\n",
        "        self._render_width = render_width\n",
        "        self._render_height = render_height\n",
        "        self._set_nested_attr(self._env, cam_dist, \"_cam_dist\")\n",
        "        self._set_nested_attr(self._env, cam_yaw, \"_cam_yaw\")\n",
        "        self._set_nested_attr(self._env, cam_pitch, \"_cam_pitch\")\n",
        "        self._set_nested_attr(self._env, render_width, \"_render_width\")\n",
        "        self._set_nested_attr(self._env, render_height, \"_render_height\")\n",
        "\n",
        "    def _set_nested_attr(self, env: gym.Env, value: int, attr: str) -> None:\n",
        "        \"\"\"\n",
        "        多重継承の属性に再帰的にアクセスして値を変更する．\n",
        "        カメラの設定に利用．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        value : int\n",
        "            設定したい値．\n",
        "        attr : str\n",
        "            変更したい属性の名前．\n",
        "        \"\"\"\n",
        "        if hasattr(env, attr):\n",
        "            setattr(env, attr, value)\n",
        "        else:\n",
        "            self._set_nested_attr(env.env, value, attr)\n",
        "\n",
        "    def __getattr(self, name: str) -> Any:\n",
        "        \"\"\"\n",
        "        環境が保持している属性値を取得するメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        name : str\n",
        "            取得したい属性値の名前．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        _env.name : Any\n",
        "            環境が保持している属性値．\n",
        "        \"\"\"\n",
        "        return getattr(self._env, name)\n",
        "\n",
        "    @property\n",
        "    def observation_space(self) -> gym.spaces.Box:\n",
        "        \"\"\"\n",
        "        観測空間に関する情報を取得するメソッド．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        space : gym.spaces.Box\n",
        "            観測空間に関する情報（各画素値の最小値，各画素値の最大値，観測データの形状， データの型）．\n",
        "        \"\"\"\n",
        "        width = self._render_width\n",
        "        height = self._render_height\n",
        "        state_space = self._env.observation_space\n",
        "        return gym.spaces.Dict({\"image\": gym.spaces.Box(0, 255, (height, width, 3), dtype=np.uint8), \"state\": state_space})\n",
        "\n",
        "    @property\n",
        "    def action_space(self) -> gym.spaces.Box:\n",
        "        \"\"\"\n",
        "        行動空間に関する情報を取得するメソッド．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        space : gym.spaces.Box\n",
        "            行動空間に関する情報（各行動の最小値，各行動の最大値，行動空間の次元， データの型） ．\n",
        "        \"\"\"\n",
        "        return self._env.action_space\n",
        "\n",
        "    # 　元の観測（低次元の状態）は今回は捨てて，env.render()で取得した画像を観測とします.\n",
        "    #  画像，報酬，終了シグナルが得られます.\n",
        "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n",
        "        \"\"\"\n",
        "        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : np.dnarray (action_dim, )\n",
        "            与える行動．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : np.ndarray (height, width, 3)\n",
        "            行動を与えたときの次の観測．\n",
        "        reward : float\n",
        "            行動を与えたときに得られる報酬．\n",
        "        done : bool\n",
        "            エピソードが終了したかどうか表すフラグ．\n",
        "        info : dict\n",
        "            その他の環境に関する情報．\n",
        "        \"\"\"\n",
        "        next_state, reward, done, info = self._env.step(action)\n",
        "        obs = {\n",
        "          \"image1\": self._env.render(mode=\"rgb_array\"),\n",
        "          \"joints\": next_state,\n",
        "        }\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        環境をリセットするためのメソッド．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : np.ndarray (height, width, 3)\n",
        "            環境をリセットしたときの初期の観測．\n",
        "        \"\"\"\n",
        "        next_state = self._env.reset()\n",
        "        obs = {\n",
        "          \"image1\": self._env.render(mode=\"rgb_array\"),\n",
        "          \"joints\": next_state,\n",
        "       }\n",
        "        return obs\n",
        "\n",
        "    def render(self, mode=\"human\", **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        観測をレンダリングするためのメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mode : str\n",
        "            レンダリング方法に関するオプション． (default='human')\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : np.ndarray (height, width, 3)\n",
        "            観測をレンダリングした結果．\n",
        "        \"\"\"\n",
        "        return self._env.render(mode, **kwargs)\n",
        "\n",
        "    def close(self) -> None:\n",
        "        \"\"\"\n",
        "        環境を閉じるためのメソッド．\n",
        "        \"\"\"\n",
        "        self._env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Bc5PvTQ_05"
      },
      "source": [
        "環境にラッパーを適用します．同時にカメラ位置に関するパラメータを与えて，カメラの位置と角度を調整します．（今回カメラのパラメータは人力で決めています．環境が変わると調整し直す必要があるかもしれません）．また，画像の大きさも同時に64x64に変更しています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkQyA1i6Q5BB"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"HalfCheetahBulletEnv-v0\")\n",
        "# カメラのパラメータを与えてカメラの位置と角度，画像の大きさを調整\n",
        "env = GymWrapper_PyBullet(\n",
        "    env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofJaPcapRiHK"
      },
      "source": [
        "もう一度画像を確認してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z33JbCqOQ-6b"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "image = env.render(mode=\"rgb_array\")\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOeH1hMgRkQq"
      },
      "source": [
        "カメラの位置が変わりました．これで元の観測より学習データとして扱いやすくなったと思います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_8zY_zSAoz"
      },
      "source": [
        "また，より環境を扱いやすくするために，同じ行動を何度か繰り返すラッパーを挟みます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaNhyssqRhXm"
      },
      "outputs": [],
      "source": [
        "class RepeatAction(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    同じ行動を指定された回数自動的に繰り返すラッパー．観測は最後の行動に対応するものになる\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: GymWrapper_PyBullet, skip: int = 4) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        env : GymWrapper_PyBullet\n",
        "            環境のインスタンス．今回は先程定義したラッパーでラップした環境を利用する．\n",
        "        skip : int\n",
        "            同じ行動を繰り返す回数．\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        環境をリセットするためのメソッド．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : np.ndarray (width, height, 3)\n",
        "            環境をリセットしたときの初期の観測．\n",
        "        \"\"\"\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n",
        "        \"\"\"\n",
        "        環境に行動を与え次の観測，報酬，終了フラグを取得するメソッド．\n",
        "        与えられた行動をskipの回数だけ繰り返した結果を返す．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : np.ndarray (action_dim, )\n",
        "            与える行動．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : np.ndarray (width, height, 3)\n",
        "            行動をskipの回数だけ繰り返したあとの観測．\n",
        "        total_reawrd : float\n",
        "            行動をskipの回数だけ繰り返したときの報酬和．\n",
        "        done : bool\n",
        "            エピソードが終了したかどうか表すフラグ．\n",
        "        info : dict\n",
        "            その他の環境に関する情報．\n",
        "        \"\"\"\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cOMwPsSSQqD"
      },
      "source": [
        "以上でラッパーに関する話は終わりです．これまでに作成したラッパーをまとめて適用し，最終的に用いる環境を作成する関数を実装して，本題のアルゴリズムの実装に移りましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q18Zg5DmSKhe"
      },
      "outputs": [],
      "source": [
        "def make_env() -> RepeatAction:\n",
        "    \"\"\"\n",
        "    作成たラッパーをまとめて適用して環境を作成する関数．\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    env : RepeatAction\n",
        "        ラッパーを適用した環境．\n",
        "    \"\"\"\n",
        "    env = gym.make(\"HalfCheetahBulletEnv-v0\")  # 環境を読み込む．今回はHalfCheetah\n",
        "    # Dreamerでは観測は64x64のRGB画像\n",
        "    env = GymWrapper_PyBullet(\n",
        "        env, cam_dist=2, cam_pitch=0, render_width=64, render_height=64\n",
        "    )\n",
        "    env = RepeatAction(env, skip=2)  # DreamerではActionRepeatは2\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC0Zt2F87ABe"
      },
      "source": [
        "# 3.RSSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUt9Lp3HMeJq"
      },
      "outputs": [],
      "source": [
        "class TransitionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    状態遷移を担うクラス．このクラスは複数の要素を含んでいます．\n",
        "    決定的状態遷移 （RNN) : h_t+1 = f(h_t, s_t, a_t)\n",
        "    確率的状態遷移による1ステップ予測として定義される \"prior\" : p(s_t+1 | h_t+1)\n",
        "    観測の情報を取り込んで定義される \"posterior\": q(s_t+1 | h_t+1, e_t+1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        rnn_hidden_dim: int,\n",
        "        ModalityInfo,\n",
        "        hidden_dim: int = 200,\n",
        "        min_stddev: float = 0.1,\n",
        "        act: \"function\" = F.elu,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state_dim : int\n",
        "            確率的状態sの次元数．\n",
        "        action_dim : int\n",
        "            行動空間の次元数．\n",
        "        rnn_hidden_dim : int\n",
        "            決定的状態遷移を計算するRNNの隠れ層の次元数．\n",
        "        ModalityInfo : tuple[Modality]\n",
        "            使用するモダリティの情報.\n",
        "        hidden_dim : int\n",
        "            決定的状態hの次元数．\n",
        "        min_stddev : float\n",
        "            確率状態遷移の標準偏差の最小値．\n",
        "        act : function\n",
        "            活性化関数．\n",
        "        \"\"\"\n",
        "        super(TransitionModel, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.rnn_hidden_dim = rnn_hidden_dim\n",
        "        self.fc_state_action = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "\n",
        "        self.fc_rnn_hidden = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc_state_mean_prior = nn.Linear(hidden_dim, state_dim)\n",
        "        self.fc_state_stddev_prior = nn.Linear(hidden_dim, state_dim)\n",
        "        # self.fc_rnn_hidden_embedded_obs = nn.Linear(rnn_hidden_dim + 1024, hidden_dim)\n",
        "        self.posteriors = nn.ModuleDict()\n",
        "        for modality in ModalityInfo:\n",
        "            self.posteriors[f'hidden_{modality.name}'] = nn.Linear(hidden_dim + modality.dim, hidden_dim)\n",
        "            self.posteriors[f'mean_{modality.name}'] = nn.Linear(hidden_dim, state_dim)\n",
        "            self.posteriors[f'stddev_{modality.name}'] = nn.Linear(hidden_dim, state_dim)\n",
        "        # self.fc_state_mean_posterior = nn.Linear(hidden_dim, state_dim)\n",
        "        # self.fc_state_stddev_posterior = nn.Linear(hidden_dim, state_dim)\n",
        "\n",
        "        # next hidden stateを計算\n",
        "        self.rnn = nn.GRUCell(hidden_dim, rnn_hidden_dim)\n",
        "        self._min_stddev = min_stddev\n",
        "        self.act = act\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        state: torch.Tensor,\n",
        "        action: torch.Tensor,\n",
        "        rnn_hidden: torch.Tensor,\n",
        "        embedded_next_obs: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        prior p(s_t+1 | h_t+1) と posterior q(s_t+1 | h_t+1, e_t+1) を返すメソッド．\n",
        "        この2つが近づくように学習する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            時刻tの状態(s_t)．\n",
        "        action : torch.Tensor (batch size, action dim)\n",
        "            時刻tの行動(a_t)．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持している決定的状態(h_t)．\n",
        "        embedded_next_obs : torch.Tensor (batch size, 1024)\n",
        "            時刻t+1の観測をエンコードしたもの(e_t+1)．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        next_state_prior : torch.Tensor (batch size, state dim)\n",
        "            prior(p(s_t+1 | h_t+1))による次の時刻の状態の予測．\n",
        "        next_state_posterior : torch.Tensor (batch size, state dim)\n",
        "            posterior(q(s_t+1 | h_t+1, e_t+1))による次の時刻の状態の予測．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持する次の決定的状態(h_t+1)．\n",
        "        \"\"\"\n",
        "        next_state_prior, rnn_hidden = self.prior(\n",
        "            self.recurrent(state, action, rnn_hidden) # h_t+1\n",
        "        )\n",
        "        next_state_posterior = self.posterior(rnn_hidden, embedded_next_obs)\n",
        "        return next_state_prior, next_state_posterior, rnn_hidden\n",
        "\n",
        "    def recurrent(\n",
        "        self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        決定的状態 h_t+1 = f(h_t, s_t, a_t)を計算するメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            時刻tの状態(s_t)．\n",
        "        action : torch.Tensor (batch size, action dim)\n",
        "            時刻tの行動(a_t)．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持している決定的状態(h_t)．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持する次の決定的状態(h_t+1)．\n",
        "        \"\"\"\n",
        "        hidden = self.act(self.fc_state_action(torch.cat([state, action], dim=1)))\n",
        "        # h_t+1を求める\n",
        "        rnn_hidden = self.rnn(hidden, rnn_hidden)\n",
        "        return rnn_hidden\n",
        "\n",
        "    def prior(self, rnn_hidden: torch.Tensor) -> Tuple[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        prior p(s_t+1 | h_t+1) を計算するメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持している決定的状態(h_t+1)．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            決定的状態を用いてサンプリングされた確率的な状態(s_t+1)．\n",
        "            ここでは決定的状態h_t+1からガウス分布の平均，標準偏差を推定してサンプリングしています．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持する決定的状態(h_t+1)．\n",
        "            入力からのものをそのまま返しています．\n",
        "        \"\"\"\n",
        "        #h_t+1を求める（ヒント: self.act, self.fc_rnn_hiddenを使用）\n",
        "        hidden = self.act(self.fc_rnn_hidden(rnn_hidden)) # WRITE ME\n",
        "\n",
        "        mean = self.fc_state_mean_prior(hidden)\n",
        "        stddev = F.softplus(self.fc_state_stddev_prior(hidden)) + self._min_stddev\n",
        "        return Normal(mean, stddev), rnn_hidden\n",
        "\n",
        "    def posterior(\n",
        "        self, rnn_hidden: torch.Tensor, modalities\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        posterior q(s_t+1 | h_t+1, e_t+1)  を計算するメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn hidden dim)\n",
        "            RNNが保持している決定的状態(h_t+1)．\n",
        "        modalities : Dict\n",
        "            画像や関節角などのモダリティ.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            決定的状態とエンコードした観測を用いてサンプリングされた確率的な状態(s_t+1)．\n",
        "            ここでは決定的状態h_t+1とエンコードした観測e_t+1からガウス分布の平均，標準偏差を推定してサンプリングしています．\n",
        "        \"\"\"\n",
        "        # h_t+1，o_t+1を結合し，q(s_t+1 | h_t+1, e_t+1) を計算する\n",
        "        posteriors = []\n",
        "        for modality_name, modality in modalities.items():\n",
        "            x = self.act(self.posteriors[f'hidden_{modality_name}'](torch.cat([rnn_hidden, modality], dim=1)))\n",
        "            mean = self.posteriors[f'mean_{modality_name}'](x) # (B, state_dim)\n",
        "            stddev = F.softplus(self.posteriors[f'stddev_{modality_name}'](x)) + self._min_stddev # (B, state_dim)\n",
        "            posteriors.append([mean, stddev])\n",
        "        # hidden = self.act(self.fc_rnn_hidden_embedded_obs(torch.cat([rnn_hidden, embedded_obs], dim=1))) # WRITE ME\n",
        "        # mean = self.fc_state_mean_posterior(hidden)\n",
        "        # stddev = F.softplus(self.fc_state_stddev_posterior(hidden)) + self._min_stddev\n",
        "\n",
        "\n",
        "        num_combinations = 2 ** len(posteriors)\n",
        "        batch_size, state_dim = mean.shape\n",
        "        PoEs = torch.zeros((num_combinations-1, batch_size, state_dim * 2), device=mean.device) # (num_comb-1, B, state_dim * 2)\n",
        "        for i in range(1, num_combinations):\n",
        "            means, stddevs = [], []\n",
        "            for j in range(len(posteriors)):\n",
        "                if (i >> j) & 1:\n",
        "                    means.append(posteriors[j][0])\n",
        "                    stddevs.append(posteriors[j][1])\n",
        "            mu = 0\n",
        "            sigma_squarred = 0\n",
        "            for mean, stddev in zip(means, stddevs):\n",
        "                one_over_sig_squarred = 1 / (stddev ** 2)\n",
        "                sigma_squarred += one_over_sig_squarred # (B, state_dim)\n",
        "                mu += mean * one_over_sig_squarred # (B, state_dim)\n",
        "            sigma_squarred = 1 / sigma_squarred\n",
        "            mu = sigma_squarred * mu\n",
        "            sigma = torch.sqrt(sigma_squarred)\n",
        "            PoEs[i-1] = torch.cat([mu, sigma], dim=1)\n",
        "\n",
        "        # MoE\n",
        "        mean, stddev = PoEs[:, :, :state_dim], PoEs[:, :, state_dim:]\n",
        "        mean = mean.mean(dim=0)\n",
        "        stddev = stddev.pow(2).sum(dim=0).sqrt() / (num_combinations - 1)\n",
        "        return Normal(mean, stddev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k4Pcf0qMeJq"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmsUzBFYMeJq"
      },
      "outputs": [],
      "source": [
        "class ObservationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    p(o_t | s_t, h_t)\n",
        "    低次元の状態表現から画像を再構成するデコーダ (3, 64, 64)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ModalityInfo, state_dim: int, rnn_hidden_dim: int, hidden_dim : int = 400) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        ModalityInfo : Tuple[Modality]\n",
        "            使用するモダリティの情報.\n",
        "        state_dim : int\n",
        "            確率的状態sの次元数．\n",
        "        rnn_hidden_dim : int\n",
        "            決定的状態hの次元数．\n",
        "        hidden_dim : int\n",
        "            隠れ層の次元数.\n",
        "        \"\"\"\n",
        "        super(ObservationModel, self).__init__()\n",
        "        self.decoders = nn.ModuleDict()\n",
        "        for modality in ModalityInfo:\n",
        "            if modality.type_ == 'image':\n",
        "                self.decoders[modality.name] = nn.Sequential(\n",
        "                    nn.Linear(state_dim + rnn_hidden_dim, 1024),\n",
        "                    nn.Unflatten(1, (1024, 1, 1)),\n",
        "                    nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.ConvTranspose2d(32, 3, kernel_size=6, stride=2),\n",
        "                )\n",
        "            elif modality.type_ == 'joint':\n",
        "                self.decoders[modality.name] = nn.Sequential(\n",
        "                    nn.Linear(state_dim + rnn_hidden_dim, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(hidden_dim, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(hidden_dim, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(hidden_dim, modality.dim)\n",
        "                )\n",
        "        # self.fc = nn.Linear(state_dim + rnn_hidden_dim, 1024)\n",
        "        # self.dc1 = nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2)\n",
        "        # self.dc2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2)\n",
        "        # self.dc3 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2)\n",
        "        # self.dc4 = nn.ConvTranspose2d(32, 3, kernel_size=6, stride=2)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor, modalities) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．確率的状態sと決定的状態hから観測を再構成する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            確率的状態s．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn_hidden_dim)\n",
        "            決定的状態h．\n",
        "        modalities : Dict\n",
        "            画像や関節角などのモダリティ.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : Dict\n",
        "            再構成された観測（画像と関節角)．\n",
        "        \"\"\"\n",
        "        obs = nn.ModuleDict()\n",
        "        for modality_name, modality in modalities.items():\n",
        "            obs[modality_name] = self.decoders[modality_name](torch.cat([state, rnn_hidden], dim=1))\n",
        "        # hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n",
        "        # hidden = hidden.view(hidden.size(0), 1024, 1, 1)\n",
        "        # hidden = F.relu(self.dc1(hidden))\n",
        "        # hidden = F.relu(self.dc2(hidden))\n",
        "        # hidden = F.relu(self.dc3(hidden))\n",
        "        # obs = self.dc4(hidden)\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duKz6wCto9Fa"
      },
      "source": [
        "## Reward Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c0l4yGSo9Fk"
      },
      "outputs": [],
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"\n",
        "    p(r_t | s_t, h_t)\n",
        "    低次元の状態表現から報酬を予測する．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        rnn_hidden_dim: int,\n",
        "        hidden_dim: int = 400,\n",
        "        act: \"function\" = F.elu,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state_dim : int\n",
        "            確率的状態sの次元数．\n",
        "        rnn_hidden_dim : int\n",
        "            決定的状態hの次元数．\n",
        "        hidden_dim : int\n",
        "            報酬モデルの隠れ層の次元数． (default=400)\n",
        "        act : function\n",
        "            報酬モデルに利用される活性化関数． (default=torch.nn.functional.elu)\n",
        "        \"\"\"\n",
        "        super(RewardModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．確率的状態sと決定的状態hから報酬rを推定する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            確率的状態s．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn_hidden_dim)\n",
        "            決定的状態h．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        reward : torch.Tensor (batch size, 1)\n",
        "            確率的状態s，決定的状態hに対する報酬r．\n",
        "        \"\"\"\n",
        "        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = self.act(self.fc2(hidden))\n",
        "        hidden = self.act(self.fc3(hidden))\n",
        "        reward = self.fc4(hidden)\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNNwwpaMYBi"
      },
      "source": [
        "## rssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzJB9ZiOxMnn"
      },
      "outputs": [],
      "source": [
        "class RSSM:\n",
        "    \"\"\"\n",
        "    TransitionModel, ObservationModel, RewardModelの3つをまとめたRSSMクラス．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        rnn_hidden_dim: int,\n",
        "        ModalityInfo,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state_dim : int\n",
        "            確率的状態sの次元数．\n",
        "        action_dim : int\n",
        "            行動空間の次元数．\n",
        "        rnn_hidden_dim : int\n",
        "            決定的状態hの次元数．\n",
        "        \"\"\"\n",
        "        self.transition = TransitionModel(state_dim, action_dim, rnn_hidden_dim, ModalityInfo).to(\n",
        "            device\n",
        "        )\n",
        "        self.observation = ObservationModel(\n",
        "            ModalityInfo,\n",
        "            state_dim,\n",
        "            rnn_hidden_dim,\n",
        "        ).to(device)\n",
        "        self.reward = RewardModel(\n",
        "            state_dim,\n",
        "            rnn_hidden_dim,\n",
        "        ).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhlVvU3aZ5FC"
      },
      "source": [
        "# 4.補助機能の実装\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsflgJ127YsG"
      },
      "source": [
        "## ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmptCR67aYZc"
      },
      "outputs": [],
      "source": [
        "# 　今回のReplayBuffer\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"\n",
        "    RNNを用いて訓練するのに適したリプレイバッファ．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, capacity: int, ModalityInfo, action_dim: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        capacity : int\n",
        "            リプレイバッファにためておくことができる経験の上限．\n",
        "        ModalityInfo : Tuple[Modality]\n",
        "            モダリティの情報．\n",
        "        action_dim : int\n",
        "            行動空間の次元数．\n",
        "        \"\"\"\n",
        "        self.capacity = capacity\n",
        "\n",
        "        self.observations = {}\n",
        "        for modality in ModalityInfo:\n",
        "            if modality.type_ == 'image':\n",
        "                self.observations[modality.name] = np.zeros(\n",
        "                    (capacity, *modality.image_shape), dtype=np.uint8\n",
        "                ) # (capacity, H, W, C)\n",
        "            elif modality.type_ == 'joint':\n",
        "                self.observations[modality.name] = np.zeros(\n",
        "                    (capacity, modality.dim), dtype=np.float32\n",
        "                ) # (capacity, joint_dim)\n",
        "        # self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8) # (capacity, H, W, C)\n",
        "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32) # (capacity, action_dim)\n",
        "        self.rewards = np.zeros((capacity, 1), dtype=np.float32) # (capacity, 1)\n",
        "        self.done = np.zeros((capacity, 1), dtype=bool) # (capacity, 1)\n",
        "        # self.done = np.zeros((capacity, 1), dtype=np.bool)\n",
        "\n",
        "        self.index = 0\n",
        "        self.is_filled = False\n",
        "\n",
        "    def push(\n",
        "        self, modalities, action: np.ndarray, reward: float, done: bool\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        リプレイバッファに経験を追加するメソッド．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        modalities : Dict\n",
        "            画像や関節角などのモダリティ.\n",
        "        action : np.ndarray (action_dim, )\n",
        "            エージェントがとった（もしくは経験を貯める際のランダムな）行動．\n",
        "        reward : float\n",
        "            観測に対して行動をとったときに得られる報酬．\n",
        "        done : bool\n",
        "            エピソードが終了するかどうかのフラグ．\n",
        "        \"\"\"\n",
        "        for modality_name, modality in modalities.items():\n",
        "            self.observations[modality_name][self.index] = modality\n",
        "        # self.observations[self.index] = observation\n",
        "        self.actions[self.index] = action\n",
        "        self.rewards[self.index] = reward\n",
        "        self.done[self.index] = done\n",
        "\n",
        "        # indexは巡回し，最も古い経験を上書きする\n",
        "        if self.index == self.capacity - 1:\n",
        "            self.is_filled = True\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size: int, chunk_length: int):\n",
        "        \"\"\"\n",
        "        経験をリプレイバッファからサンプルします．（ほぼ）一様なサンプルです．\n",
        "        結果として返ってくるのは観測（画像），行動，報酬，終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です．\n",
        "        各バッチは連続した経験になっています．\n",
        "        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            バッチサイズ．\n",
        "        chunk_length : int\n",
        "            バッチあたりの系列長．\n",
        "\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sampled_observations : Dict\n",
        "            バッファからサンプリングされたモダリティ.\n",
        "        sampled_actions : np.ndarray (batch size, chunk length, action dim)\n",
        "            バッファからサンプリングされた行動．\n",
        "        sampled_rewards : np.ndarray (batch size, chunk length, 1)\n",
        "            バッファからサンプリングされた報酬．\n",
        "        sampled_rewards : np.ndarray (batch size, chunk length, 1)\n",
        "            バッファからサンプリングされたエピソードの終了フラグ．\n",
        "        \"\"\"\n",
        "        episode_borders = np.where(self.done)[0]\n",
        "        sampled_indexes = []\n",
        "        for _ in range(batch_size):\n",
        "            cross_border = True\n",
        "            while cross_border:\n",
        "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
        "                final_index = initial_index + chunk_length - 1\n",
        "                cross_border = np.logical_and(\n",
        "                    initial_index <= episode_borders, episode_borders < final_index\n",
        "                ).any()  # 論理積\n",
        "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
        "\n",
        "        sampled_observations = {}\n",
        "        for modality_name, modality in self.observations.items():\n",
        "            sampled_observations[modality_name] = self.observations[modality_name][sampled_indexes].reshape(\n",
        "                batch_size, chunk_length, *self.observations[modality_name].shape[1:] # (len(sampled_idnex), H, W, C) -> (B, chunk_length, H, W, C)\n",
        "            )\n",
        "\n",
        "        # sampled_observations = self.observations[sampled_indexes].reshape(\n",
        "        #     batch_size, chunk_length, *self.observations.shape[1:] # (len(sampled_idnex), H, W, C) -> (B, chunk_length, H, W, C)\n",
        "        # )\n",
        "\n",
        "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, self.actions.shape[1]\n",
        "        )\n",
        "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1\n",
        "        )\n",
        "        sampled_done = self.done[sampled_indexes].reshape(batch_size, chunk_length, 1)\n",
        "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        バッファに貯められている経験の数を返すメソッド．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        length : int\n",
        "            バッファに貯められている経験の数．\n",
        "        \"\"\"\n",
        "        return self.capacity if self.is_filled else self.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLIt0qiya1_2"
      },
      "outputs": [],
      "source": [
        "def preprocess_obs(obs: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    画像を正規化する．[0, 255] -> [-0.5, 0.5]．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n",
        "        環境から得られた観測．画素値は[0, 255]．\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    normalized_obs : np.ndarray (64, 64, 3) or (chank length, batch size, 64, 64, 3)\n",
        "        画素値を[-0.5, 0.5]で正規化した観測．\n",
        "    \"\"\"\n",
        "    obs = obs.astype(np.float32)\n",
        "    normalized_obs = obs / 255.0 - 0.5\n",
        "    return normalized_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD5tQXKea4lA"
      },
      "source": [
        "Dreamerでは価値関数の学習を行いますが，このために通常のTD誤差ではなく，**TD(λ)をベースにしたλ-return**としてターゲット価値を計算し，それと現在の予測価値の誤差を用います．そのためにλ-returnを計算する関数をここで実装しておきます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLZMKJj_bIHr"
      },
      "outputs": [],
      "source": [
        "def lambda_target(\n",
        "    rewards: torch.Tensor, values: torch.Tensor, gamma: float, lambda_: float\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    価値関数の学習のためのλ-returnを計算する関数．\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rewards : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
        "        報酬モデルによる報酬の推定値．\n",
        "    values : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
        "        価値関数を近似するValueモデルによる状態価値観数の推定値．\n",
        "    gamma : float\n",
        "        割引率．\n",
        "    lambda_ : float\n",
        "        λ-returnのパラメータλ．\n",
        "\n",
        "    V_lambda : torch.Tensor (imagination_horizon, batch size * (chank length - 1))\n",
        "        各状態に対するλ-returnの値．\n",
        "    \"\"\"\n",
        "    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n",
        "    H = rewards.shape[0] - 1\n",
        "    V_n = torch.zeros_like(rewards, device=rewards.device)\n",
        "    V_n[H] = values[H]\n",
        "    for n in range(1, H + 1):\n",
        "        # まずn-step returnを計算します\n",
        "        # 注意: 系列が途中で終わってしまったら，可能な中で最大のnを用いたn-stepを使います\n",
        "        V_n[:-n] = (gamma**n) * values[n:]\n",
        "        for k in range(1, n + 1):\n",
        "            if k == n:\n",
        "                V_n[:-n] += (gamma ** (n - 1)) * rewards[k:]\n",
        "            else:\n",
        "                V_n[:-n] += (gamma ** (k - 1)) * rewards[k : -n + k]\n",
        "\n",
        "        # lambda_でn-step returnを重みづけてλ-returnを計算します\n",
        "        if n == H:\n",
        "            V_lambda += (lambda_ ** (H - 1)) * V_n\n",
        "        else:\n",
        "            V_lambda += (1 - lambda_) * (lambda_ ** (n - 1)) * V_n\n",
        "\n",
        "    return V_lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keAW0J-qS24Q"
      },
      "source": [
        "# 6.Dreamerの実装\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_ARrbicUZzP"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGY-YZzmSh-3"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    (3, 64, 64)の画像を(1024,)のベクトルに変換するエンコーダクラス．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "        層の定義のみを行う．\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.cv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2)\n",
        "        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.cv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2)\n",
        "        self.cv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．観測画像をベクトルに埋め込む．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        obs : torch.Tensor (batch size, 3, 64, 64)\n",
        "            環境から得られた観測画像．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        embedded_obs : torch.Tensor (batch size, 1024)\n",
        "            観測を1024次元のベクトルに埋め込んだもの．\n",
        "        \"\"\"\n",
        "        hidden = F.relu(self.cv1(obs))\n",
        "        hidden = F.relu(self.cv2(hidden))\n",
        "        hidden = F.relu(self.cv3(hidden))\n",
        "        embedded_obs = F.relu(self.cv4(hidden)).reshape(hidden.size(0), -1)\n",
        "        return embedded_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7coVKT3AWMx3"
      },
      "source": [
        "ここからがDreamerの中核となる部分で，RSSMの学習を通して獲得された低次元の状態表現の上でActor-Criticを行います.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVY3TCdWx2L"
      },
      "source": [
        "以下で，価値関数を近似するValueモデル $v_{\\phi}(s_{\\tau})　\\approx E_{q(.|s_{\\tau})}(\\sum_{\\tau=t}^{t+H}(\\gamma^{\\tau-t}r_{\\tau}) )$ を実装します．Q学習などで用いられる状態行動価値関数Q(s, a)ではなく，状態価値関数V(s)であることに多少の注意が必要です．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gegVfLVWLLR"
      },
      "outputs": [],
      "source": [
        "class ValueModel(nn.Module):\n",
        "    \"\"\"\n",
        "    低次元の状態表現(state_dim + rnn_hidden_dim)から状態価値を出力するクラス．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        rnn_hidden_dim: int,\n",
        "        hidden_dim: int = 400,\n",
        "        act: \"function\" = F.elu,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state_dim : int\n",
        "            確率的状態sの次元数．\n",
        "        rnn_hidden_dim : int\n",
        "            決定的状態hの次元数．\n",
        "        hidden_dim : int\n",
        "            モデルの隠れ層の次元数． (default=400)\n",
        "        act : function\n",
        "            モデルの活性化関数． (default=torch.nn.functional.elu)\n",
        "        \"\"\"\n",
        "        super(ValueModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．低次元の状態表現から状態価値を推定する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor (batch size, state dim)\n",
        "            確率的状態s．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn_hidden_dim)\n",
        "            決定的状態h．\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state_value : torch.Tensor (batch size, 1)\n",
        "            入力された状態に対する状態価値の推定値．\n",
        "        \"\"\"\n",
        "        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = self.act(self.fc2(hidden))\n",
        "        hidden = self.act(self.fc3(hidden))\n",
        "        state_value = self.fc4(hidden)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adOXp2TRXQ_Y"
      },
      "source": [
        "最後です．実際に行動を出力するActionモデル $a_{\\tau} \\sim q_{\\delta}(a_{\\tau}|s_{\\tau})$ を実装します．\n",
        "\n",
        "Actionモデルは価値の見積もりを最大化することを目的とします．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2iSa_-kW_W7"
      },
      "outputs": [],
      "source": [
        "class ActionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    低次元の状態表現(state_dim + rnn_hidden_dim)から行動を計算するクラス．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        rnn_hidden_dim: int,\n",
        "        action_dim: int,\n",
        "        hidden_dim: int = 400,\n",
        "        act: \"function\" = F.elu,\n",
        "        min_stddev: float = 1e-4,\n",
        "        init_stddev: float = 5.0,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state_dim : int\n",
        "            確率的状態sの次元数．\n",
        "        rnn_hidden_dim : int\n",
        "            決定的状態hの次元数．\n",
        "        action_dim : int\n",
        "            行動空間の次元数．\n",
        "        hidden_dim : int\n",
        "            モデルの隠れ層の次元数． (default=400)\n",
        "        act : function\n",
        "            モデルの活性化関数． (default=torch.nn.functional.elu)\n",
        "        min_stddev : float\n",
        "            行動をサンプリングする分布の標準偏差の最小値． (default=1e-4)\n",
        "        init_stddev : float\n",
        "            行動をサンプリングする分布の標準偏差の初期値． (default=5.0)\n",
        "        \"\"\"\n",
        "        super(ActionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.fc_stddev = nn.Linear(hidden_dim, action_dim)\n",
        "        self.act = act\n",
        "        self.min_stddev = min_stddev\n",
        "        self.init_stddev = np.log(np.exp(init_stddev) - 1)\n",
        "\n",
        "    def forward(\n",
        "        self, state: torch.Tensor, rnn_hidden: torch.Tensor, training: bool = True\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        順伝播を行うメソッド．入力された状態に対する行動を出力する．\n",
        "        training=Trueなら，NNのパラメータに関して微分可能な形の行動のサンプル（Reparametrizationによる）を返す．\n",
        "        training=Falseなら，行動の確率分布の平均値を返す．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        staet : torch.Tensor (batch size, state dim)\n",
        "            確率的状態s．\n",
        "        rnn_hidden : torch.Tensor (batch size, rnn_hidden_dim)\n",
        "            決定的状態h．\n",
        "        training : bool\n",
        "            訓練か推論かを示すフラグ． (default=True)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action : torch.Tensor (batch size, action dim)\n",
        "            入力された状態に対する行動．\n",
        "            training=Trueでは微分可能な形の行動をサンプリングした値，\n",
        "            training=Falseでは行動の確率分布の平均値を返す．\n",
        "        \"\"\"\n",
        "        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = self.act(self.fc2(hidden))\n",
        "        hidden = self.act(self.fc3(hidden))\n",
        "        hidden = self.act(self.fc4(hidden))\n",
        "\n",
        "        # Dreamerの実装に合わせて少し平均と分散に対する簡単な変換が入っています\n",
        "        mean = self.fc_mean(hidden)\n",
        "        mean = 5.0 * torch.tanh(mean / 5.0)\n",
        "        stddev = self.fc_stddev(hidden)\n",
        "        stddev = F.softplus(stddev + self.init_stddev) + self.min_stddev\n",
        "\n",
        "        if training:\n",
        "            action = torch.tanh(Normal(mean, stddev).rsample())  # 微分可能にするためrsample()\n",
        "        else:\n",
        "            action = torch.tanh(mean)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5L46nAhYAAJ"
      },
      "source": [
        "実装の詳細まで掴みきれなくとも，個々のクラスが担っている役割が大雑把にでもわかっていただければ幸いです."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdKZ1S2_boOd"
      },
      "source": [
        "# 7.エージェントの実装\n",
        "\n",
        "Dreamerでは行動を計算するために低次元の状態表現が必要で，この状態表現はRSSMを用いて計算されるため，テスト時もこの状態表現のためにRSSMによる推論を行い続ける必要があります．\n",
        "\n",
        "そのため，先ほど実装したActionModelをそのまま使っても簡単には行動を決定できません．\n",
        "\n",
        "ここを扱いやすくするために，RSSMを使って低次元の状態表現を計算しつつ，行動を決定するAgentクラスを実装します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dof4rokgYj-6"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    ActionModelに基づき行動を決定する．そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス．\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder: Encoder, rssm: RSSM, action_model: ActionModel) -> None:\n",
        "        \"\"\"\n",
        "        コンストラクタ．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        encoder : Encoder\n",
        "            上で定義したEncoderクラスのインスタンス．\n",
        "            観測画像を1024次元のベクトルに埋め込む ．\n",
        "        rssm : RSSM\n",
        "            上で定義したRSSMクラスのインスタンス．\n",
        "            遷移モデル，1024次元のベクトルを観測画像にするデコーダ，報酬を予測するモデルを持つ．\n",
        "        action_model : ActionModel\n",
        "            上で定義したActionModelのインスタンス．\n",
        "            低次元の状態表現から行動を予測する．\n",
        "        \"\"\"\n",
        "        self.encoder = encoder\n",
        "        self.rssm = rssm\n",
        "        self.action_model = action_model\n",
        "\n",
        "        self.device = next(self.action_model.parameters()).device\n",
        "        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n",
        "\n",
        "    def __call__(self, modalities, training=True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        特殊メソッド．\n",
        "        インスタンスに直接引数を渡すことで実行される．\n",
        "        （例）agent = Agent(*args)\n",
        "             action = agent(obs)  # このときに__call__メソッドが呼び出される．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        modalities : Dict # np.ndarray (batch size, 3, 64, 64)\n",
        "            環境から得られた観測画像．\n",
        "        training : bool\n",
        "            訓練か推論かを示すフラグ． (default=True)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action : np.ndarray (batch size, action dim)\n",
        "            入力された観測に対する行動の予測．\n",
        "        \"\"\"\n",
        "        # preprocessを適用，PyTorchのためにChannel-Firstに変換\n",
        "        for modality_name, modality in modalities.items():\n",
        "            if Modality.get_type(modality_name) == 'image':\n",
        "                modality = preprocess_obs(modality)\n",
        "                modality = np.transpose(modality, (2, 0, 1)) # (1, C, H, W)\n",
        "            modality = np.expand_dims(modality, axis=0)\n",
        "            modalities[modality_name] = torch.as_tensor(modality, device=self.device)\n",
        "\n",
        "\n",
        "        # obs = preprocess_obs(obs)\n",
        "        # obs = torch.as_tensor(obs, device=self.device)\n",
        "        # obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0) # (1, C, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 観測を低次元の表現に変換し，posteriorからのサンプルをActionModelに入力して行動を決定する\n",
        "            for modality_name, modality in modalities.items():\n",
        "                if Modality.get_type(modality_name) == 'image':\n",
        "                    modalities[modality_name] = self.encoder(modality)\n",
        "\n",
        "            state_posterior = self.rssm.posterior(self.rnn_hidden, modalities)\n",
        "            state = state_posterior.sample()\n",
        "            # embedded_obs = self.encoder(obs)\n",
        "            # state_posterior = self.rssm.posterior(self.rnn_hidden, embedded_obs)\n",
        "            # state = state_posterior.sample()\n",
        "            action = self.action_model(state, self.rnn_hidden, training=training) # (1, action_dim)\n",
        "\n",
        "            # 次のステップのためにRNNの隠れ状態を更新しておく\n",
        "            _, self.rnn_hidden = self.rssm.prior(\n",
        "                self.rssm.recurrent(state, action, self.rnn_hidden)\n",
        "            )\n",
        "\n",
        "        return action.squeeze().cpu().numpy() # (action_dim, )\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"\n",
        "        RNNの隠れ状態（=決定的状態）をリセットする．\n",
        "        \"\"\"\n",
        "        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUPWAkpyc9Z-"
      },
      "source": [
        "# 8.ハイパーパラメータの設定と学習の準備\n",
        "\n",
        "ここまででDreamerの基本的な構成要素は実装が終わりました．あとはハイパーパラメータを設定し，モデルやリプレイバッファを宣言して学習の準備を整えます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwWkQyVLWxdD"
      },
      "outputs": [],
      "source": [
        "class Modality:\n",
        "    num2type = {}\n",
        "\n",
        "    def __init__(self, name, dim, type_):\n",
        "        self.name = name # モダリティの識別番号\n",
        "        self.dim = dim # 画像の場合はembedされた後のdim, 関節角の場合はそのまま\n",
        "        self.type_ = type_ # 'image' or 'joint'\n",
        "        if type_ == 'image':\n",
        "            self.image_shape = (64, 64, 3)\n",
        "        Modality.num2type[name] = type_\n",
        "\n",
        "    @classmethod\n",
        "    def get_type(cls, number):\n",
        "        return cls.num2type[number]\n",
        "\n",
        "ModalityInfo = (\n",
        "    Modality('image1', 1024, 'image'),\n",
        "    Modality('joints', 26, 'joint'),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8sW8K5Kdjrg"
      },
      "outputs": [],
      "source": [
        "# リプレイバッファの宣言\n",
        "buffer_capacity = 200000  # Colabのメモリの都合上，元の実装より小さめにとっています\n",
        "replay_buffer = ReplayBuffer(\n",
        "    capacity=buffer_capacity,\n",
        "    ModalityInfo=ModalityInfo,\n",
        "    action_dim=env.action_space.shape[0],\n",
        ")\n",
        "\n",
        "# モデルの宣言\n",
        "state_dim = 30  # 確率的状態の次元\n",
        "rnn_hidden_dim = 200  # 決定的状態（RNNの隠れ状態）の次元\n",
        "# 確率的状態の次元と決定的状態（RNNの隠れ状態）の次元は一致しなくて良い\n",
        "encoder = Encoder().to(device)\n",
        "rssm = RSSM(\n",
        "    state_dim,\n",
        "    env.action_space.shape[0], # shapeが(6, )なのでshape[0]は6\n",
        "    rnn_hidden_dim,\n",
        "    ModalityInfo,\n",
        ")\n",
        "value_model = ValueModel(state_dim, rnn_hidden_dim).to(device)\n",
        "action_model = ActionModel(state_dim, rnn_hidden_dim, env.action_space.shape[0]).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "# オプティマイザの宣言\n",
        "model_lr = 6e-4  # encoder, rssm, obs_model, reward_modelの学習率\n",
        "value_lr = 8e-5\n",
        "action_lr = 8e-5\n",
        "eps = 1e-4\n",
        "model_params = (\n",
        "    list(encoder.parameters())\n",
        "    + list(rssm.transition.parameters())\n",
        "    + list(rssm.observation.parameters())\n",
        "    + list(rssm.reward.parameters())\n",
        ")\n",
        "model_optimizer = torch.optim.Adam(model_params, lr=model_lr, eps=eps)\n",
        "value_optimizer = torch.optim.Adam(value_model.parameters(), lr=value_lr, eps=eps)\n",
        "action_optimizer = torch.optim.Adam(action_model.parameters(), lr=action_lr, eps=eps)\n",
        "\n",
        "# その他ハイパーパラメータ\n",
        "seed_episodes = 5  # 最初にランダム行動で探索するエピソード数\n",
        "all_episodes = 100  # 学習全体のエピソード数（300ほどで，ある程度収束します）\n",
        "test_interval = 10  # 何エピソードごとに探索ノイズなしのテストを行うか\n",
        "model_save_interval = 20  # NNの重みを何エピソードごとに保存するか\n",
        "collect_interval = 100  # 何回のNNの更新ごとに経験を集めるか（＝1エピソード経験を集めるごとに何回更新するか）\n",
        "\n",
        "action_noise_var = 0.3  # 探索ノイズの強さ\n",
        "\n",
        "batch_size = 50\n",
        "chunk_length = 50  # 1回の更新で用いる系列の長さ\n",
        "imagination_horizon = 15  # Actor-Criticの更新のために，Dreamerで何ステップ先までの想像上の軌道を生成するか\n",
        "\n",
        "\n",
        "gamma = 0.9  # 割引率\n",
        "lambda_ = 0.95  # λ-returnのパラメータ\n",
        "clip_grad_norm = 100  # gradient clippingの値\n",
        "free_nats = 3  # KL誤差（RSSMのTransitionModelにおけるpriorとposteriorの間の誤差）がこの値以下の場合，無視する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUoI0l9Ee0Tp"
      },
      "source": [
        "# 9.学習\n",
        "まず，最初の数エピソードはランダムに行動して経験をリプレイバッファに貯めます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txKI9CCne5e3"
      },
      "outputs": [],
      "source": [
        "env = make_env() # Wrapされたenv\n",
        "for episode in range(seed_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "        replay_buffer.push(obs, action, reward, done)\n",
        "        obs = next_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsE7Hu6yfW2B"
      },
      "source": [
        "学習結果を確認するために，TensorBoardを立ち上げておきます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fmFqkidfEXP"
      },
      "outputs": [],
      "source": [
        "log_dir = \"./logs\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "%tensorboard --logdir './logs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raG5bSzCfrhV"
      },
      "source": [
        "以下がメインの学習ループです．それぞれのコメントを見て，実装の内容を追ってください.\n",
        "\n",
        "学習にはColab Proで3時間半ぐらいの時間がかかります."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZp9GS1D3Hj"
      },
      "source": [
        "## トレーニング回す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_LNryZQfnKT",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for episode in range(seed_episodes, 20):\n",
        "    # -----------------------------\n",
        "    # 各エピソードから経験を集める\n",
        "    # -----------------------------\n",
        "    start = time.time()\n",
        "    # 行動を決定するためのエージェントを宣言\n",
        "    policy = Agent(encoder, rssm.transition, action_model) # rssm.transitionはTransitionクラス\n",
        "\n",
        "    env = make_env()\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = policy(obs) # __call__を呼び出し。 (action_dim, )\n",
        "        # 探索のためにガウス分布に従うノイズを加える(explaration noise)\n",
        "        action += np.random.normal(0, np.sqrt(action_noise_var),\n",
        "                                     env.action_space.shape[0])\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        #リプレイバッファに観測，行動，報酬，doneを格納\n",
        "        replay_buffer.push(obs, action, reward, done)\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "    # 訓練時の報酬と経過時間をログとして表示\n",
        "    writer.add_scalar('total reward at train', total_reward, episode)\n",
        "    print('episode [%4d/%4d] is collected. Total reward is %f' %\n",
        "            (episode+1, all_episodes, total_reward))\n",
        "    print('elasped time for interaction: %.2fs' % (time.time() - start))\n",
        "\n",
        "    # NNのパラメータを更新する\n",
        "    start = time.time()\n",
        "    for update_step in range(collect_interval): # 1エピソード集めたときcollect_interval回NN更新する\n",
        "        # -------------------------------------------------------------------------------------\n",
        "        #  RSSM(trainsition_model, obs_model, reward_model)の更新 - Dynamics learning\n",
        "        #  事前分布と事後分布のKL距離、再構成誤差、報酬モデルの３つの損失関数を用いてDynamicsを学習する\n",
        "        # -------------------------------------------------------------------------------------\n",
        "        observations, actions, rewards, _ = \\\n",
        "            replay_buffer.sample(batch_size, chunk_length) # (B, chunk_length, H, W, C), (B, chunk_length, action_dim), ...\n",
        "\n",
        "        # 観測を前処理し，RNNを用いたPyTorchでの学習のためにTensorの次元を調整\n",
        "        for modality_name, modality in observations.items():\n",
        "            if Modality.get_type(modality_name) == 'image':\n",
        "                modality = preprocess_obs(modality)\n",
        "                modality = rearrange(modality, \"b t h w c -> t b c h w\")  # (T, B, C, H, W)\n",
        "            elif Modality.get_type(modality_name) == 'joint':\n",
        "                modality = rearrange(modality, \"b t d -> t b d\") # (T, B, joint_dim)\n",
        "            observations[modality_name] = torch.as_tensor(modality, device=device)  # (T, B, *)\n",
        "\n",
        "        # observations = preprocess_obs(observations)\n",
        "        # observations = torch.as_tensor(observations, device=device)  # (B, T, H, W, C)\n",
        "        actions = torch.as_tensor(actions, device=device)  # (B, T, action dim)\n",
        "        rewards = torch.as_tensor(rewards, device=device)  # (B, T, 1)\n",
        "\n",
        "        # observations = rearrange(observations, \"b t h w c -> t b c h w\")  # (T, B, C, H, W)\n",
        "        actions = rearrange(actions, \"b t d -> t b d\")  # (T, B, action dim)\n",
        "        rewards = rearrange(rewards, \"b t d -> t b d\")  # (T, B, 1)\n",
        "\n",
        "        # 観測をエンコーダで低次元のベクトルに変換\n",
        "        embedded_observations = encoder(\n",
        "            observations.reshape(-1, 3, 64, 64)).view(chunk_length, batch_size, -1)  # (T, B, 1024)\n",
        "\n",
        "        # 低次元の状態表現を保持しておくためのTensorを定義\n",
        "        states = torch.zeros(chunk_length, batch_size, state_dim, device=device)  # (T, B, state dim)\n",
        "        rnn_hiddens = torch.zeros(chunk_length, batch_size, rnn_hidden_dim, device=device)  # (T, B, rnn hidden dim)\n",
        "\n",
        "        # 低次元の状態表現は最初はゼロ初期化（timestep１つ分）\n",
        "        state = torch.zeros(batch_size, state_dim, device=device)\n",
        "        rnn_hidden = torch.zeros(batch_size, rnn_hidden_dim, device=device)\n",
        "\n",
        "        #KL loss#################################################################################\n",
        "        # 状態s_tの予測を行ってそのロスを計算する（priorとposteriorの間のKLダイバージェンス）\n",
        "        kl_loss = 0\n",
        "        for l in range(chunk_length-1):\n",
        "            next_state_prior, next_state_posterior, rnn_hidden = \\\n",
        "                rssm.transition(state, actions[l], rnn_hidden, embedded_observations[l+1])  # (B, state_dim)\n",
        "            state = next_state_posterior.rsample()\n",
        "            states[l+1] = state\n",
        "            rnn_hiddens[l+1] = rnn_hidden\n",
        "            kl = kl_divergence(next_state_prior, next_state_posterior).sum(dim=1) # WRITE ME （ヒント: kl_divergence()を使用）多変量正規分布の各次元を足す\n",
        "            kl_loss += kl.clamp(min=free_nats).mean()  # 原論文通り，KL誤差がfree_nats以下の時は無視\n",
        "        kl_loss /= (chunk_length - 1)\n",
        "        ##########################################################################################\n",
        "\n",
        "        # states[0] and rnn_hiddens[0]はゼロ初期化なので以降では使わない\n",
        "        # states，rnn_hiddensは低次元の状態表現\n",
        "        states = states[1:]  # (T-1, B, state dim)\n",
        "        rnn_hiddens = rnn_hiddens[1:]  # (T-1, B, rnn hidden dim)\n",
        "\n",
        "        #obs lossとrecon loss#####################################################################\n",
        "        # 観測を再構成，また，報酬を予測\n",
        "        flatten_states = states.view(-1, state_dim)  # ((T-1) x B, state dim)\n",
        "        flatten_rnn_hiddens = rnn_hiddens.view(-1, rnn_hidden_dim)  # ((T-1) x B, rnn hidden dim)\n",
        "        recon_observations = rssm.observation(flatten_states, flatten_rnn_hiddens)\n",
        "        for modality_name, modality in recon_observations.items():\n",
        "            if Modality.get_type(modality_name) == 'image':\n",
        "                recon_observations[modality_name] = observations.view(chunk_length-1, batch_size, 3, 64, 64)\n",
        "            elif Modality.get_type(modality_name) == 'joint':\n",
        "                recon_observations[modality_name] = observations.view(chunk_length-1, batch_size, -1)\n",
        "        # recon_observations = rssm.observation(flatten_states, flatten_rnn_hiddens).view(chunk_length-1, batch_size, 3, 64, 64)  # (T-1, B, C, H, W)\n",
        "        predicted_rewards = rssm.reward(flatten_states, flatten_rnn_hiddens).view(chunk_length-1, batch_size, 1)  # (T-1, B, 1)\n",
        "\n",
        "        # 観測と報酬の予測誤差を計算\n",
        "        obs_loss = 0\n",
        "        for modality_name, recon_modality in recon_observations.items():\n",
        "            if Modality.get_type(modality_name) == 'image':\n",
        "                obs_loss += 0.5 * F.mse_loss(recon_modality, observations[modality_name][1:], reduction='none').mean([0, 1]).sum()\n",
        "            if Modality.get_type(modality_name) == 'joint':\n",
        "                obs_loss += 0.5 * F.mse_loss(recon_modality, observations[modality_name][1:])\n",
        "        # obs_loss = 0.5 * F.mse_loss(recon_observations, observations[1:], reduction='none').mean([0, 1]).sum() #(T-1, B, C, H, W) -> (C, H, W) -> (1, )\n",
        "        reward_loss = 0.5 * F.mse_loss(predicted_rewards, rewards[:-1])\n",
        "        ###########################################################################################\n",
        "\n",
        "\n",
        "        # 以上のロスを合わせて勾配降下で更新する\n",
        "        model_loss = kl_loss + obs_loss + reward_loss\n",
        "        model_optimizer.zero_grad()\n",
        "        model_loss.backward()\n",
        "        clip_grad_norm_(model_params, clip_grad_norm)\n",
        "        model_optimizer.step()\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        #  Action Model, Value　Modelの更新　- Behavior learning\n",
        "        # --------------------------------------------------\n",
        "        # Actor-Criticのロスで他のモデルを更新することはないので勾配の流れを一度遮断\n",
        "        # flatten_states, flatten_rnn_hiddensは RSSMから得られた低次元の状態表現を平坦化した値\n",
        "        flatten_states = flatten_states.detach()\n",
        "        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n",
        "\n",
        "        # DreamerにおけるActor-Criticの更新のために，現在のモデルを用いた\n",
        "        # 数ステップ先の未来の状態予測を保持するためのTensorを用意\n",
        "        imagined_states = torch.zeros(imagination_horizon + 1,\n",
        "                                         *flatten_states.shape, # ((T-1) x B, state dim)\n",
        "                                          device=flatten_states.device)  # (horizon + 1, (T-1) x B, state dim)\n",
        "        imagined_rnn_hiddens = torch.zeros(imagination_horizon + 1,\n",
        "                                                *flatten_rnn_hiddens.shape,\n",
        "                                                device=flatten_rnn_hiddens.device)  # (horizon + 1, (T-1) x B, rnn hidden dim)\n",
        "\n",
        "        #　未来予測をして想像上の軌道を作る前に，最初の状態としては先ほどモデルの更新で使っていた\n",
        "        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n",
        "        imagined_states[0] = flatten_states\n",
        "        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n",
        "\n",
        "        # open-loopで未来の状態予測を使い，想像上の軌道を作る\n",
        "        for h in range(1, imagination_horizon + 1):\n",
        "            # 行動はActionModelで決定．この行動はモデルのパラメータに対して微分可能で,\n",
        "            #　これを介してActionModelは更新される\n",
        "            actions = action_model(flatten_states, flatten_rnn_hiddens)  # ((T-1) x B, action dim)\n",
        "            flatten_states_prior, flatten_rnn_hiddens = rssm.transition.prior(rssm.transition.recurrent(flatten_states,\n",
        "                                                                   actions,\n",
        "                                                                   flatten_rnn_hiddens))\n",
        "            flatten_states = flatten_states_prior.rsample()\n",
        "            imagined_states[h] = flatten_states  # ((T-1) x B, state dim)\n",
        "            imagined_rnn_hiddens[h] = flatten_rnn_hiddens  # ((T-1) x B, rnn hidden dim)\n",
        "\n",
        "        # RSSMのreward_modelにより予測された架空の軌道に対する報酬を計算\n",
        "        flatten_imagined_states = imagined_states.view(-1, state_dim)  # ((hotizon+1)x(T-1)xB, state dim)\n",
        "        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, rnn_hidden_dim)  # ((horizon+1)x(T-1)xB, rnn hidden dim)\n",
        "        imagined_rewards = \\\n",
        "            rssm.reward(flatten_imagined_states,\n",
        "                        flatten_imagined_rnn_hiddens).view(imagination_horizon + 1, -1)  # ((horizon+1), (T-1)xB, state dim)\n",
        "        imagined_values = \\\n",
        "            value_model(flatten_imagined_states,\n",
        "                        flatten_imagined_rnn_hiddens).view(imagination_horizon + 1, -1)  # ((horizon+1), (T-1)xB, rnn hidden dim)\n",
        "\n",
        "        # λ-returnのターゲットを計算(V_{\\lambda}(s_{\\tau})\n",
        "        lambda_target_values = lambda_target(imagined_rewards, imagined_values, gamma, lambda_) # WRITE ME （ヒント: lambda_target()を利用）  # ((horizon+1), (T-1)xB, 1)\n",
        "\n",
        "        # 価値関数の予測した価値が大きくなるようにActionModelを更新\n",
        "        # PyTorchの基本は勾配降下だが，今回は大きくしたいので-1をかける\n",
        "        action_loss = -lambda_target_values.mean()\n",
        "        action_optimizer.zero_grad()\n",
        "        action_loss.backward()\n",
        "        clip_grad_norm_(action_model.parameters(), clip_grad_norm)\n",
        "        action_optimizer.step()\n",
        "\n",
        "        # TD(λ)ベースの目的関数で価値関数を更新（価値関数のみを学習するため，学習しない変数のグラフは切っている．)\n",
        "        imagined_values = value_model(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(imagination_horizon + 1, -1)# ((horizon+1), (T-1)xB, 1)\n",
        "        value_loss = 0.5 * F.mse_loss(imagined_values, lambda_target_values.detach()) # WRITE ME （ヒント: 0.5 * F.mse_loss()を使用）\n",
        "        value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        clip_grad_norm_(value_model.parameters(), clip_grad_norm)\n",
        "        value_optimizer.step()\n",
        "\n",
        "        # ログをTensorBoardに出力\n",
        "        print('update_step: %3d model loss: %.5f, kl_loss: %.5f, '\n",
        "             'obs_loss: %.5f, reward_loss: %.5f, '\n",
        "             'value_loss: %.5f action_loss: %.5f'\n",
        "                % (update_step + 1, model_loss.item(), kl_loss.item(),\n",
        "                    obs_loss.item(), reward_loss.item(),\n",
        "                    value_loss.item(), action_loss.item()))\n",
        "        total_update_step = episode * collect_interval + update_step\n",
        "        writer.add_scalar('model loss', model_loss.item(), total_update_step)\n",
        "        writer.add_scalar('kl loss', kl_loss.item(), total_update_step)\n",
        "        writer.add_scalar('obs loss', obs_loss.item(), total_update_step)\n",
        "        writer.add_scalar('reward loss', reward_loss.item(), total_update_step)\n",
        "        writer.add_scalar('value loss', value_loss.item(), total_update_step)\n",
        "        writer.add_scalar('action loss', action_loss.item(), total_update_step)\n",
        "\n",
        "    print('elasped time for update: %.2fs' % (time.time() - start))\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #    テストフェーズ．探索ノイズなしでの性能を評価する\n",
        "    # --------------------------------------------------------------\n",
        "    if (episode + 1) % test_interval == 0:\n",
        "        policy = Agent(encoder, rssm.transition, action_model)\n",
        "        start = time.time()\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = policy(obs, training=False)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "        writer.add_scalar('total reward at test', total_reward, episode)\n",
        "        print('Total test reward at episode [%4d/%4d] is %f' %\n",
        "                (episode+1, all_episodes, total_reward))\n",
        "        print('elasped time for test: %.2fs' % (time.time() - start))\n",
        "\n",
        "    if (episode + 1) % model_save_interval == 0:\n",
        "        # 定期的に学習済みモデルのパラメータを保存する\n",
        "        model_log_dir = os.path.join(log_dir, 'episode_%04d' % (episode + 1))\n",
        "        os.makedirs(model_log_dir, exist_ok=True)\n",
        "        torch.save(encoder.state_dict(), os.path.join(model_log_dir, 'encoder.pth'))\n",
        "        torch.save(rssm.transition.state_dict(), os.path.join(model_log_dir, 'rssm.pth'))\n",
        "        torch.save(rssm.observation.state_dict(), os.path.join(model_log_dir, 'obs_model.pth'))\n",
        "        torch.save(rssm.reward.state_dict(), os.path.join(model_log_dir, 'reward_model.pth'))\n",
        "        torch.save(value_model.state_dict(), os.path.join(model_log_dir, 'value_model.pth'))\n",
        "        torch.save(action_model.state_dict(), os.path.join(model_log_dir, 'action_model.pth'))\n",
        "    del env\n",
        "    gc.collect()\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFMZzNPU_B-l"
      },
      "source": [
        "TensorBoardで学習結果を確認してみます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTJL-DXo-omv"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir='./logs'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3F2tAU0kepm"
      },
      "source": [
        "##  10.結果の確認\n",
        "保存された学習済み重みを用いて，動作を確認してみましょう."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnfmCGFFAV0q"
      },
      "source": [
        "学習にはかなりの時間がかかるので，ここでは事前に学習しておいた重みを読み込むことにします．時間のある方は，上記のコードで実際に学習した重みを使って同様に試してみてください."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGif3TtyilSY"
      },
      "outputs": [],
      "source": [
        "# # 事前にGoogle Driveにあげておいた学習済み重みをダウンロードします\n",
        "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "# file_id_dreamer = \"1EE6okFLo33RlUAmLdwsCvkXql3hqbjVu\"  # Google Driveにあげた学習済重みのfile idを取得してここにコピペしてください\n",
        "# file_id_mopoedreamer = \"1r0YXULm4xBoSql0sxzE1GhWuTQyMQkeM\"\n",
        "# gdd.download_file_from_google_drive(\n",
        "#     file_id=file_id_dreamer, dest_path=\"./Dreamer\", unzip=True, overwrite=True\n",
        "# )\n",
        "# gdd.download_file_from_google_drive(\n",
        "#     file_id=file_id_mopoedreamer, dest_path=\"./MoPoE-Dreamer\", unzip=True, overwrite=True\n",
        "# )\n",
        "\n",
        "# # import zipfile\n",
        "\n",
        "# # def unzip_file(zip_file_path, extract_to_path):\n",
        "# #   \"\"\"\n",
        "# #   zipファイルを指定されたディレクトリに解凍する。\n",
        "\n",
        "# #   Args:\n",
        "# #     zip_file_path: 解凍するzipファイルのパス。\n",
        "# #     extract_to_path: 解凍先のディレクトリパス。\n",
        "# #   \"\"\"\n",
        "# #   with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "# #     zip_ref.extractall(extract_to_path)\n",
        "\n",
        "# # # 使用例\n",
        "# # zip_file_path_dreamer = './Dreamer/episode_0300.zip'  # ダウンロードしたzipファイルのパス\n",
        "# # extract_to_path_dreamer = './Dreamer/'  # 解凍先のディレクトリ (カレントディレクトリに解凍)\n",
        "# # zip_file_path_mopoedreamer = './MoPoE-Dreamer/episode_0300.zip'  # ダウンロードしたzipファイルのパス\n",
        "# # extract_to_path_mopoedreamer = './MoPoE-Dreamer/'\n",
        "\n",
        "# # unzip_file(zip_file_path_dreamer, extract_to_path_dreamer)\n",
        "# # unzip_file(zip_file_path_mopoedreamer, extract_to_path_mopoedreamer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfjpZAegyrz8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xyuj-kAzvVx"
      },
      "outputs": [],
      "source": [
        "# dir = '/content/drive/MyDrive/世界モデル/最終課題/Parameters/DreamerV1'\n",
        "dir = '/content/drive/MyDrive/世界モデル/最終課題/Parameters/MoPoE-DreamerV1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGI8xV_qp0Ud"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder().to(device)\n",
        "rssm = RSSM(\n",
        "    state_dim,\n",
        "    env.action_space.shape[0], # shapeが(6, )なのでshape[0]は6\n",
        "    rnn_hidden_dim,\n",
        "    ModalityInfo,\n",
        ")\n",
        "value_model = ValueModel(state_dim, rnn_hidden_dim).to(device)\n",
        "action_model = ActionModel(state_dim, rnn_hidden_dim, env.action_space.shape[0]).to(\n",
        "    device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKYF5zP6ALD3"
      },
      "outputs": [],
      "source": [
        "encoder.load_state_dict(torch.load(os.path.join(dir, 'episode_0300/encoder.pth'), map_location=device))\n",
        "rssm.transition.load_state_dict(torch.load(os.path.join(dir, \"episode_0300/rssm.pth\"), map_location=device))\n",
        "rssm.observation.load_state_dict(torch.load(os.path.join(dir, \"episode_0300/obs_model.pth\"), map_location=device))\n",
        "action_model.load_state_dict(torch.load(os.path.join(dir, \"episode_0300/action_model.pth\"), map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSt_P-WdEnmO"
      },
      "outputs": [],
      "source": [
        "# # 学習済み重みを用いず，このcolab上で学習したモデルを使うなら，このセルを実行してください．\n",
        "# # あるいは，定期的に保存されているモデルを読み込むこともできます\n",
        "# encoder.load_state_dict(torch.load(os.path.join(model_log_dir, \"encoder.pth\")))\n",
        "# rssm.transition.load_state_dict(torch.load(os.path.join(model_log_dir, \"rssm.pth\")))\n",
        "# rssm.observation.load_state_dict(\n",
        "#     torch.load(os.path.join(model_log_dir, \"obs_model.pth\"))\n",
        "# )\n",
        "# action_model.load_state_dict(\n",
        "#     torch.load(os.path.join(model_log_dir, \"action_model.pth\"))\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fCAGLBjGaDz"
      },
      "source": [
        "動作の様子を動画で観てみることにします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tsrk0bfEXpO"
      },
      "outputs": [],
      "source": [
        "env = make_env()\n",
        "policy = Agent(encoder, rssm.transition, action_model)\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs['image1']]\n",
        "while not done:\n",
        "    action = policy(obs, training=False)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    frames.append(obs['image1'])\n",
        "\n",
        "print(\"Total Reward:\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phTlvvC8GZdU"
      },
      "outputs": [],
      "source": [
        "# 結果を動画で観てみるための関数\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from matplotlib import animation\n",
        "\n",
        "\n",
        "def display_video(frames: List[np.ndarray]) -> None:\n",
        "    \"\"\"\n",
        "    結果を動画にするための関数．\n",
        "\n",
        "    frames : List[np.ndarray]\n",
        "        観測画像をリスト化したもの．\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 8), dpi=50)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        plt.title(\"Step %d\" % (i))\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    display(HTML(anim.to_jshtml(default_mode=\"once\")))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apt5aPsd4nTN"
      },
      "outputs": [],
      "source": [
        "# full\n",
        "import statistics\n",
        "reward_list = []\n",
        "env = make_env()\n",
        "policy = Agent(encoder, rssm.transition, action_model)\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs['image1']]\n",
        "sum_total_reward = 0\n",
        "iter = 10\n",
        "for _ in range(iter):\n",
        "    while not done:\n",
        "        action = policy(obs, training=False)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        frames.append(obs['image1'])\n",
        "\n",
        "    print(\"Total Reward:\", total_reward)\n",
        "    reward_list.append(total_reward)\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "print(f'average reward: {statistics.mean(reward_list)} \\n std: {statistics.stdev(reward_list)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x7znbFqM3XTS"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLesUGof4z6t"
      },
      "outputs": [],
      "source": [
        "# only img\n",
        "reward_list = []\n",
        "def extract_image(obs):\n",
        "    new_obs = {}\n",
        "    for modality_name, modality in obs.items():\n",
        "        if modality_name == 'image1':\n",
        "            new_obs[modality_name] = modality\n",
        "    return new_obs\n",
        "\n",
        "env = make_env()\n",
        "policy = Agent(encoder, rssm.transition, action_model)\n",
        "obs = env.reset()\n",
        "obs = extract_image(obs)\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs['image1']]\n",
        "iter = 10\n",
        "for _ in range(iter):\n",
        "    while not done:\n",
        "        action = policy(obs, training=False)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        obs = extract_image(obs)\n",
        "        total_reward += reward\n",
        "        frames.append(obs['image1'])\n",
        "\n",
        "    print(\"Total Reward:\", total_reward)\n",
        "    reward_list.append(total_reward)\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "print(f'average reward: {statistics.mean(reward_list)} \\n std: {statistics.stdev(reward_list)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN-6pLFxG6sz"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-F_LHWN-R1I"
      },
      "outputs": [],
      "source": [
        "# only join\n",
        "reward_list = []\n",
        "def extract_joints(obs):\n",
        "    new_obs = {}\n",
        "    for modality_name, modality in obs.items():\n",
        "        if modality_name == 'joints':\n",
        "            new_obs[modality_name] = modality\n",
        "    return new_obs\n",
        "\n",
        "env = make_env()\n",
        "policy = Agent(encoder, rssm.transition, action_model)\n",
        "obs = env.reset()\n",
        "new_obs = extract_joints(obs)\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs['image1']]\n",
        "iter = 10\n",
        "for _ in range(iter):\n",
        "    while not done:\n",
        "        action = policy(new_obs, training=False)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        new_obs = extract_joints(obs)\n",
        "        total_reward += reward\n",
        "        frames.append(obs['image1'])\n",
        "\n",
        "    print(\"Total Reward:\", total_reward)\n",
        "    reward_list.append(total_reward)\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "print(f'average reward: {statistics.mean(reward_list)} \\n std: {statistics.stdev(reward_list)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5xt3Yo-5_SNH"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62nJ7xoek-RU"
      },
      "source": [
        "ある時点の適当な観測から，世界モデルで**open-loop**に未来予測を行わせ，観測を再構成して視覚的に観てみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUYvdbwSlDo_"
      },
      "outputs": [],
      "source": [
        "policy = Agent(encoder, rssm.transition, action_model)\n",
        "obs = env.reset()\n",
        "# 最初に適当な回数行動します．この間にrnn_hiddenに観測の系列に関する情報が蓄積されます\n",
        "for _ in range(np.random.randint(5, 100)):\n",
        "    action = policy(obs, training=False)\n",
        "    obs, _, _, _ = env.step(action)\n",
        "\n",
        "# 現在の観測をベクトルに変換し，それを元にposteriorを計算します．\n",
        "preprocessed_obs = preprocess_obs(obs)\n",
        "preprocessed_obs = torch.as_tensor(preprocessed_obs, device=device)\n",
        "preprocessed_obs = preprocessed_obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n",
        "with torch.no_grad():\n",
        "    embedded_obs = encoder(preprocessed_obs)\n",
        "\n",
        "# posteriorからのサンプルとして得られたstateと，policyから取得したrnn_hiddenが低次元の状態表現です．\n",
        "# open-loopの予測なので，これ以降この2つの変数は状態遷移を表すpriorでしか更新しません．\n",
        "# （policyの中では，行動を決定するために観測をリアルタイムで反映してposteriorで更新しています）\n",
        "rnn_hidden = policy.rnn_hidden\n",
        "state = rssm.transition.posterior(rnn_hidden, embedded_obs).sample()\n",
        "frame = np.zeros((64, 128, 3))\n",
        "frames = []\n",
        "\n",
        "prediction_length = 100\n",
        "for _ in range(prediction_length):\n",
        "    action = policy(obs)\n",
        "    obs, _, _, _ = env.step(action)\n",
        "\n",
        "    action = torch.as_tensor(action, device=device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        state_prior, rnn_hidden = rssm.transition.prior(\n",
        "            rssm.transition.recurrent(state, action, rnn_hidden)\n",
        "        )\n",
        "        state = state_prior.sample()\n",
        "        predicted_obs = rssm.observation(\n",
        "            state, rnn_hidden\n",
        "        )  # obs_model(state, rnn_hidden)\n",
        "\n",
        "    frame[:, :64, :] = preprocess_obs(obs)\n",
        "    frame[:, 64:, :] = (\n",
        "        predicted_obs.squeeze().transpose(0, 1).transpose(1, 2).cpu().numpy()\n",
        "    )\n",
        "    frames.append((frame + 0.5).clip(0.0, 1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeH7vX0zLS-P"
      },
      "source": [
        "open-loopの動画予測の結果を，左側に真のフレーム，右側に予測されたフレームと並べてみてみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG9EWSvrDFJ0"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K0slk_dLPVJ"
      },
      "source": [
        "以上で演習は終わりです．お疲れ様でした！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_PCEOJ69prx"
      },
      "source": [
        "## 11.参考文献\n",
        "[[1]](https://arxiv.org/pdf/1811.04551.pdf) Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, \"Learning Latent Dynamics for Planning from Pixels\", arXiv, 2019\n",
        "\n",
        "[[2]](https://arxiv.org/abs/1912.01603) Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi,\n",
        "\"Dream to Control: Learning Behaviors by Latent Imagination\", ICLR2020"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1c906a337007ca492b40f9e66323e61f3dcaf71886120485625fb02da1be1aa9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}